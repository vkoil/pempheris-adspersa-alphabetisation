{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "---\n",
    "Selecting and converting segments into a master feature table for clustering.\n",
    "Note: not all features are/need to be used to fit the chosen clustering algorithm.\n",
    "Chosen features can be filtered from this master table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from librosa import feature\n",
    "from spafe.features import lpc\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000 # sample rate\n",
    "\n",
    "input_data_dir = 'data/segmented/tank/'\n",
    "output_data_dir = 'data/segmented/'\n",
    "\n",
    "# discard any segment that does not have an amplitude at threshold or higher.\n",
    "# This gets rid of non-content signals or pure noise segments.\n",
    "segment_threshold = 0.0005\n",
    "min_duration = sr * 0.010 # get rid of short segments (less than 10ms)\n",
    "\n",
    "ffl = 128 # window size used to compute features\n",
    "fhl = 32 # hop length used to compute features\n",
    "lpcc_order = 10 # order for lpc coffecient filter equal to number of coefficients\n",
    "\n",
    "assert min_duration > ffl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Feature Table for Clustering from Segmented Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(output_data_dir, \"segment_features.csv\")):\n",
    "    os.remove(os.path.join(output_data_dir, \"segment_features.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(columns=[\"file_id\", \"file_len_s\",\n",
    "\"zcr_mean\", \"zcr_std\", \"zcr_max\", \"zcr_min\",\n",
    "\"spc_centroid_mean\", \"spc_centroid_std\", \"spc_centroid_max\", \"spc_centroid_min\",\n",
    "\"spc_bandwidth_mean\", \"spc_bandwidth_std\", \"spc_bandwidth_max\", \"spc_bandwidth_min\",\n",
    "\"spc_contrast_mean\", \"spc_contrast_std\", \"spc_contrast_max\", \"spc_contrast_min\",\n",
    "\"spc_flatness_mean\", \"spc_flatness_std\", \"spc_flatness_max\", \"spc_flatness_min\",\n",
    "\"spc_rolloff_mean\", \"spc_rolloff_std\", \"spc_rolloff_max\", \"spc_rolloff_min\",\n",
    "\"rms_mean\", \"rms_std\", \"rms_max\", \"rms_min\",\n",
    "\"mfcc_sum_0\",\"mfcc_sum_1\",\"mfcc_sum_2\",\"mfcc_sum_3\",\"mfcc_sum_4\",\"mfcc_sum_5\",\"mfcc_sum_6\",\"mfcc_sum_7\",\"mfcc_sum_8\",\"mfcc_sum_9\",\n",
    "\"mfcc_mean_0\",\"mfcc_mean_1\",\"mfcc_mean_2\",\"mfcc_mean_3\",\"mfcc_mean_4\",\"mfcc_mean_5\",\"mfcc_mean_6\",\"mfcc_mean_7\",\"mfcc_mean_8\",\"mfcc_mean_9\",\n",
    "\"mfcc_std_0\",\"mfcc_std_1\",\"mfcc_std_2\",\"mfcc_std_3\",\"mfcc_std_4\",\"mfcc_std_5\",\"mfcc_std_6\",\"mfcc_std_7\",\"mfcc_std_8\",\"mfcc_std_9\",\n",
    "\"mfcc_max_0\",\"mfcc_max_1\",\"mfcc_max_2\",\"mfcc_max_3\",\"mfcc_max_4\",\"mfcc_max_5\",\"mfcc_max_6\",\"mfcc_max_7\",\"mfcc_max_8\",\"mfcc_max_9\",\n",
    "\"mfcc_min_0\",\"mfcc_min_1\",\"mfcc_min_2\",\"mfcc_min_3\",\"mfcc_min_4\",\"mfcc_min_5\",\"mfcc_min_6\",\"mfcc_min_7\",\"mfcc_min_8\",\"mfcc_min_9\",\n",
    "\"lpcc_sum_0\",\"lpcc_sum_1\",\"lpcc_sum_2\",\"lpcc_sum_3\",\"lpcc_sum_4\",\"lpcc_sum_5\",\"lpcc_sum_6\",\"lpcc_sum_7\",\"lpcc_sum_8\",\"lpcc_sum_9\",\n",
    "\"lpcc_mean_0\",\"lpcc_mean_1\",\"lpcc_mean_2\",\"lpcc_mean_3\",\"lpcc_mean_4\",\"lpcc_mean_5\",\"lpcc_mean_6\",\"lpcc_mean_7\",\"lpcc_mean_8\",\"lpcc_mean_9\",\n",
    "\"lpcc_std_0\",\"lpcc_std_1\",\"lpcc_std_2\",\"lpcc_std_3\",\"lpcc_std_4\",\"lpcc_std_5\",\"lpcc_std_6\",\"lpcc_std_7\",\"lpcc_std_8\",\"lpcc_std_9\",\n",
    "\"lpcc_max_0\",\"lpcc_max_1\",\"lpcc_max_2\",\"lpcc_max_3\",\"lpcc_max_4\",\"lpcc_max_5\",\"lpcc_max_6\",\"lpcc_max_7\",\"lpcc_max_8\",\"lpcc_max_9\",\n",
    "\"lpcc_min_0\",\"lpcc_min_1\",\"lpcc_min_2\",\"lpcc_min_3\",\"lpcc_min_4\",\"lpcc_min_5\",\"lpcc_min_6\",\"lpcc_min_7\",\"lpcc_min_8\",\"lpcc_min_9\"\n",
    "]).to_csv(os.path.join(output_data_dir, \"segment_features.csv\"), index=False, header=True)\n",
    "\n",
    "for f in os.listdir(input_data_dir):\n",
    "    segment, _ = librosa.load(os.path.join(input_data_dir, f), sr=sr) \n",
    "\n",
    "    if max(segment) >= segment_threshold and len(segment) >= min_duration:\n",
    "\n",
    "        segment = minmax_scale(X=segment, feature_range=(-0.1,0.1)) #normalise each segment\n",
    "\n",
    "        zcr = feature.zero_crossing_rate(segment, frame_length=ffl, hop_length=fhl)\n",
    "        spc_centroid = feature.spectral_centroid(y=segment, n_fft=ffl, center=False, sr=sr)\n",
    "        spc_bandwidth = feature.spectral_bandwidth(y=segment, n_fft=ffl, hop_length=fhl, center=False, sr=sr)\n",
    "        spc_contrast = feature.spectral_contrast(y=segment, n_fft=ffl, hop_length=fhl, center=False, sr=sr)\n",
    "        spc_flatness = feature.spectral_flatness(y=segment, n_fft=ffl, hop_length=fhl, center=False)\n",
    "        spc_rolloff = feature.spectral_rolloff(y=segment, n_fft=ffl, hop_length=fhl, center=False, sr=sr)\n",
    "        rms = feature.rms(y=segment, frame_length=ffl, hop_length=fhl)\n",
    "        mfcc = pd.DataFrame(feature.mfcc(y=segment, sr=sr, n_mfcc=10, n_fft=ffl, n_mels=10, fmax=700)).T\n",
    "        lpcc = pd.DataFrame(lpc.lpcc(segment, fs=sr, order=lpcc_order, win_len=ffl/sr, win_hop=fhl/sr))\n",
    "\n",
    "        pd.DataFrame([\n",
    "        f,\n",
    "        len(segment)/sr,\n",
    "\n",
    "        np.mean(zcr),\n",
    "        np.std(zcr),\n",
    "        np.max(zcr),\n",
    "        np.min(zcr),\n",
    "\n",
    "        np.mean(spc_centroid),\n",
    "        np.std(spc_centroid),\n",
    "        np.max(spc_centroid),\n",
    "        np.min(spc_centroid),\n",
    "\n",
    "        np.mean(spc_bandwidth),\n",
    "        np.std(spc_bandwidth),\n",
    "        np.max(spc_bandwidth),\n",
    "        np.min(spc_bandwidth),\n",
    "\n",
    "        np.mean(spc_contrast),\n",
    "        np.std(spc_contrast),\n",
    "        np.max(spc_contrast),\n",
    "        np.min(spc_contrast),\n",
    "\n",
    "        np.mean(spc_flatness),\n",
    "        np.std(spc_flatness),\n",
    "        np.max(spc_flatness),\n",
    "        np.min(spc_flatness),\n",
    "\n",
    "        np.mean(spc_rolloff),\n",
    "        np.std(spc_rolloff),\n",
    "        np.max(spc_rolloff),\n",
    "        np.min(spc_rolloff),\n",
    "\n",
    "        np.mean(rms),\n",
    "        np.std(rms),\n",
    "        np.max(rms),\n",
    "        np.min(rms),\n",
    "\n",
    "        mfcc.sum()[0],\n",
    "        mfcc.sum()[1],\n",
    "        mfcc.sum()[2],\n",
    "        mfcc.sum()[3],\n",
    "        mfcc.sum()[4],\n",
    "        mfcc.sum()[5],\n",
    "        mfcc.sum()[6],\n",
    "        mfcc.sum()[7],\n",
    "        mfcc.sum()[8],\n",
    "        mfcc.sum()[9],\n",
    "\n",
    "        mfcc.mean()[0],\n",
    "        mfcc.mean()[1],\n",
    "        mfcc.mean()[2],\n",
    "        mfcc.mean()[3],\n",
    "        mfcc.mean()[4],\n",
    "        mfcc.mean()[5],\n",
    "        mfcc.mean()[6],\n",
    "        mfcc.mean()[7],\n",
    "        mfcc.mean()[8],\n",
    "        mfcc.mean()[9],\n",
    "\n",
    "        mfcc.std(ddof=0)[0],\n",
    "        mfcc.std(ddof=0)[1],\n",
    "        mfcc.std(ddof=0)[2],\n",
    "        mfcc.std(ddof=0)[3],\n",
    "        mfcc.std(ddof=0)[4],\n",
    "        mfcc.std(ddof=0)[5],\n",
    "        mfcc.std(ddof=0)[6],\n",
    "        mfcc.std(ddof=0)[7],\n",
    "        mfcc.std(ddof=0)[8],\n",
    "        mfcc.std(ddof=0)[9],\n",
    "\n",
    "        mfcc.max()[0],\n",
    "        mfcc.max()[1],\n",
    "        mfcc.max()[2],\n",
    "        mfcc.max()[3],\n",
    "        mfcc.max()[4],\n",
    "        mfcc.max()[5],\n",
    "        mfcc.max()[6],\n",
    "        mfcc.max()[7],\n",
    "        mfcc.max()[8],\n",
    "        mfcc.max()[9],\n",
    "\n",
    "        mfcc.min()[0],\n",
    "        mfcc.min()[1],\n",
    "        mfcc.min()[2],\n",
    "        mfcc.min()[3],\n",
    "        mfcc.min()[4],\n",
    "        mfcc.min()[5],\n",
    "        mfcc.min()[6],\n",
    "        mfcc.min()[7],\n",
    "        mfcc.min()[8],\n",
    "        mfcc.min()[9],\n",
    "\n",
    "        lpcc.sum()[0],\n",
    "        lpcc.sum()[1],\n",
    "        lpcc.sum()[2],\n",
    "        lpcc.sum()[3],\n",
    "        lpcc.sum()[4],\n",
    "        lpcc.sum()[5],\n",
    "        lpcc.sum()[6],\n",
    "        lpcc.sum()[7],\n",
    "        lpcc.sum()[8],\n",
    "        lpcc.sum()[9],\n",
    "\n",
    "        lpcc.mean()[0],\n",
    "        lpcc.mean()[1],\n",
    "        lpcc.mean()[2],\n",
    "        lpcc.mean()[3],\n",
    "        lpcc.mean()[4],\n",
    "        lpcc.mean()[5],\n",
    "        lpcc.mean()[6],\n",
    "        lpcc.mean()[7],\n",
    "        lpcc.mean()[8],\n",
    "        lpcc.mean()[9],\n",
    "\n",
    "        lpcc.std(ddof=0)[0],\n",
    "        lpcc.std(ddof=0)[1],\n",
    "        lpcc.std(ddof=0)[2],\n",
    "        lpcc.std(ddof=0)[3],\n",
    "        lpcc.std(ddof=0)[4],\n",
    "        lpcc.std(ddof=0)[5],\n",
    "        lpcc.std(ddof=0)[6],\n",
    "        lpcc.std(ddof=0)[7],\n",
    "        lpcc.std(ddof=0)[8],\n",
    "        lpcc.std(ddof=0)[9],\n",
    "\n",
    "        lpcc.max()[0],\n",
    "        lpcc.max()[1],\n",
    "        lpcc.max()[2],\n",
    "        lpcc.max()[3],\n",
    "        lpcc.max()[4],\n",
    "        lpcc.max()[5],\n",
    "        lpcc.max()[6],\n",
    "        lpcc.max()[7],\n",
    "        lpcc.max()[8],\n",
    "        lpcc.max()[9],\n",
    "\n",
    "        lpcc.min()[0],\n",
    "        lpcc.min()[1],\n",
    "        lpcc.min()[2],\n",
    "        lpcc.min()[3],\n",
    "        lpcc.min()[4],\n",
    "        lpcc.min()[5],\n",
    "        lpcc.min()[6],\n",
    "        lpcc.min()[7],\n",
    "        lpcc.min()[8],\n",
    "        lpcc.min()[9]\n",
    "        \n",
    "        ]).T.to_csv(os.path.join(output_data_dir, \"segment_features.csv\"), index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Final Output Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(output_data_dir, \"segment_features.csv\")).drop(columns=[\"file_id\", \"file_len_s\"])\n",
    "\n",
    "assert np.all(np.isfinite(df)) # check that there are no infinite values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30c044526e5b867a9e6fcb8cc6575f33acc42df1bdb3b43eea9f4e61a35fc699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
